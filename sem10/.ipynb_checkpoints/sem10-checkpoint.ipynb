{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:18.309671Z",
     "start_time": "2019-09-28T19:10:14.654448Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engeneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда взять признаки?\n",
    "\n",
    "1) Преобразовать исходные признаки (повороты, вращения, расстояния, переход в другую систему координат; месяцы, дни недели, часы открытия и т.п.; расстояние/время и др.)\n",
    "\n",
    "2) Создать календарь с праздниками, Черными пятницами, днями голосований и другими важными событиями\n",
    "\n",
    "3) Подгрузить географические данные: координаты общественных мест, остановок,  аэропортов и т.п.\n",
    "\n",
    "4) Напарсить чего-нибудь в Интернете: отзывы, цены в каталогах, рейтинги продуктов\n",
    "\n",
    "5) Найти похожий датасет с новыми фичами\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с текстовыми данными\n",
    "\n",
    "Как правило, модели машинного обучения действуют в предположении, что матрица \"объект-признак\" является вещественнозначной, поэтому при работе с текстами сперва для каждого из них необходимо составить его признаковое описание. Для этого широко используются техники векторизации, tf-idf и пр. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:18.959288Z",
     "start_time": "2019-09-28T19:10:18.346983Z"
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all', categories=['comp.graphics', 'sci.med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные содержат тексты новостей, которые надо классифицировать на разделы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:18.987312Z",
     "start_time": "2019-09-28T19:10:18.964475Z"
    }
   },
   "outputs": [],
   "source": [
    "data['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:19.012942Z",
     "start_time": "2019-09-28T19:10:18.997929Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = data['data']\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:19.060074Z",
     "start_time": "2019-09-28T19:10:19.019838Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words (сколько раз слово из словаря входило в данный текст)\n",
    "\n",
    "Самый очевидный способ формирования признакового описания текстов — векторизация. Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
    "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
    "\n",
    "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:19.833837Z",
     "start_time": "2019-09-28T19:10:19.175011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "[[0 1 1 1]\n",
      " [0 2 1 0]\n",
      " [1 0 0 1]\n",
      " [1 0 0 0]]\n",
      "1\n",
      "{'dog': 2, 'cat': 1, 'fish': 3, 'bird': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())\n",
    "print(cv_fit.toarray())\n",
    "print(cv.get_feature_names().index('cat'))\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# разбить предложение на токены и подсчитывает число повторений этого токена в документе\n",
    "vectorizer = CountVectorizer(encoding='utf8')\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результатом является разреженная матрица."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = vectorizer.transform(texts[:1]) # закодировать один документ как вектор (нужна не строка, а лист)\n",
    "\n",
    "#vec1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ на задачку: max(vec1.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:19.872060Z",
     "start_time": "2019-09-28T19:10:19.854688Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vec1.indptr)\n",
    "print(vec1.indices)\n",
    "print(vec1.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Ещё один способ работы с текстовыми данными — [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (**T**erm **F**requency–**I**nverse **D**ocument **F**requency). Рассмотрим коллекцию текстов $D$.  Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n",
    "\n",
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте:\n",
    "$$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$\n",
    "где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "1. Inverse Document Frequency\n",
    "$$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$\n",
    "где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n",
    "\n",
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину:\n",
    "\n",
    "$$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Смысл в том, чтобы дать больший вес тем словам, которые характерны для конкретного документа, но не для всех документов вообще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:20.548074Z",
     "start_time": "2019-09-28T19:10:19.877266Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=1)\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем разреженную матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:20.569113Z",
     "start_time": "2019-09-28T19:10:20.550328Z"
    }
   },
   "outputs": [],
   "source": [
    "vec2 = vectorizer.transform(texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:20.601343Z",
     "start_time": "2019-09-28T19:10:20.574776Z"
    }
   },
   "outputs": [],
   "source": [
    "print(vec2.indptr)\n",
    "print(vec2.indices)\n",
    "print(vec2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что одно и то же слово может встречаться в различных формах (например, \"сотрудник\" и \"сотрудника\"), но описанные выше методы интерпретируют их как различные слова, что делает признаковое описание избыточным. Устранить эту проблему можно при помощи **лемматизации** и **стемминга**.\n",
    "\n",
    "### Стемминг\n",
    "\n",
    "[**Stemming**](https://en.wikipedia.org/wiki/Stemming) –  это процесс нахождения основы слова. В результате применения данной процедуры однокоренные слова, как правило, преобразуются к одинаковому виду.\n",
    "\n",
    "**Примеры стемминга:**\n",
    "\n",
    "| Word        | Stem           |\n",
    "| ----------- |:-------------:|\n",
    "| вагон | вагон |\n",
    "| вагона | вагон |\n",
    "| вагоне | вагон |\n",
    "| вагонов | вагон |\n",
    "| вагоном | вагон |\n",
    "| вагоны | вагон |\n",
    "| важная | важн |\n",
    "| важнее | важн |\n",
    "| важнейшие | важн |\n",
    "| важнейшими | важн |\n",
    "| важничал | важнича |\n",
    "| важно | важн |\n",
    "\n",
    "[Snowball](http://snowball.tartarus.org/) – фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке – списки окончаний для разных чистей речи, разных склонений и т.д. Пример алгоритма для русского языка – [Russian stemming](http://snowballstem.org/algorithms/russian/stemmer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:21.109437Z",
     "start_time": "2019-09-28T19:10:20.604439Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.snowball.RussianStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:21.130340Z",
     "start_time": "2019-09-28T19:10:21.112772Z"
    }
   },
   "outputs": [],
   "source": [
    "#пример\n",
    "print(stemmer.stem('приветливый'), stemmer.stem('приветик'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_eng = nltk.stem.snowball.EnglishStemmer()\n",
    "stemmer_eng.stem('ridiculous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:25.274844Z",
     "start_time": "2019-09-28T19:10:21.135760Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer_eng = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "def stem_text(text, stemmer):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(map(lambda w: stemmer.stem(w), tokens))\n",
    "\n",
    "stemmed_texts = []\n",
    "for t in tqdm(texts):\n",
    "    stemmed_texts.append(stem_text(t, stemmer_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:25.290119Z",
     "start_time": "2019-09-28T19:10:25.279611Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:25.306834Z",
     "start_time": "2019-09-28T19:10:25.294638Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(stemmed_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, стеммер работает не очень быстро и запускать его для всей выборки достаточно накладно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "[Лемматизация](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, для русского языка есть библиотека pymorphy2.\n",
    "В ней для морфологического анализа слов (русских) есть класс MorphAnalyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:25.442948Z",
     "start_time": "2019-09-28T19:10:25.311429Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод MorphAnalyzer.parse() принимает слово  и возвращает все возможные разборы слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse('делавших')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним работу стеммера и лемматизатора на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:25.489016Z",
     "start_time": "2019-09-28T19:10:25.474203Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "print(stemmer.stem('играющих'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse('играющих')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T19:10:25.502206Z",
     "start_time": "2019-09-28T19:10:25.493051Z"
    }
   },
   "outputs": [],
   "source": [
    "print(morph.parse('играющих')[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, у каждого разбора есть тег.\n",
    "\n",
    "Тег - это набор граммем, характеризующих данное слово. Например, тег 'VERB,perf,intr plur,past,indc' означает, что слово - глагол (VERB) совершенного вида (perf), непереходный (intr), множественного числа (plur), прошедшего времени (past), изъявительного наклонения (indc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse('играющих')[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privetiki = morph.parse('приветики')[0]\n",
    "privetiki.inflect({'gent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privetiki.inflect({'plur', 'gent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применение в задаче классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectorizer\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3)\n",
    "for train_ids, test_ids in cv.split(X):\n",
    "    lr = LogisticRegression(solver='lbfgs')\n",
    "    lr.fit(X[train_ids], target[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(target[test_ids], preds), \n",
    "                                        accuracy_score(target[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(encoding='utf8', min_df=1)\n",
    "vectorizer_tfidf.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "X = vectorizer_tfidf.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ids, test_ids in cv.split(X):\n",
    "    lr = LogisticRegression(solver='lbfgs')\n",
    "    lr.fit(X[train_ids], target[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(target[test_ids], preds), \n",
    "                                        accuracy_score(target[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer_tfidf.transform(stemmed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ids, test_ids in cv.split(X):\n",
    "    lr = LogisticRegression(solver='lbfgs')\n",
    "    lr.fit(X[train_ids], target[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(target[test_ids], preds), \n",
    "                                        accuracy_score(target[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence = 'Между прочим, вы сегодня более сообразительны, чем вчера'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "word_tokens = word_tokenize(my_sentence) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens =  word_tokenize(text) \n",
    "    filtered_text = [w for w in tokens if not w in stop_words] \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "filtered_text = []\n",
    "for t in tqdm(texts):\n",
    "    filtered_text.append(remove_stop_words(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer_tfidf.transform(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_ids, test_ids in cv.split(X):\n",
    "    lr = LogisticRegression(solver='lbfgs')\n",
    "    lr.fit(X[train_ids], target[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(target[test_ids], preds), \n",
    "                                        accuracy_score(target[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация текстов\n",
    "\n",
    "#### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_im = wc.WordCloud(width = 3000, height = 2000, random_state=1, background_color='grey', collocations=False, stopwords = stopwords.words('english')).generate(texts[0])\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.imshow(wc_im) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scattertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scattertext as st\n",
    "import spacy\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speach_df = st.SampleCorpora.ConventionData2012.get_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = st.CorpusFromPandas(speach_df, \n",
    "                              category_col='party', \n",
    "                              text_col='text',\n",
    "                            nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(corpus.get_scaled_f_scores_vs_background().index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df['Democratic Score'] = corpus.get_scaled_f_scores('democrat')\n",
    "print(list(term_freq_df.sort_values(by='Democratic Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(corpus,\n",
    "          category='democrat',\n",
    "          category_name='Democratic',\n",
    "          not_category_name='Republican',\n",
    "          width_in_pixels=1000,\n",
    "          metadata=speach_df['speaker'])\n",
    "\n",
    "open(\"my_best_vis.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача предсказания длительности позедки такси"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_features = list(train.columns)\n",
    "y = train.trip_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание данных:\n",
    "\n",
    " - **id** - a unique identifier for each trip\n",
    " - **vendor_id** - a code indicating the provider associated with the trip record\n",
    " - **pickup_datetime** - date and time when the meter was engaged\n",
    " - **dropoff_datetime** - date and time when the meter was disengaged\n",
    " - **passenger_count** - the number of passengers in the vehicle (driver entered value)\n",
    " - **pickup_longitude** - the longitude where the meter was engaged\n",
    " - **pickup_latitude** - the latitude where the meter was engaged\n",
    " - **dropoff_longitude** - the longitude where the meter was disengaged\n",
    " - **dropoff_latitude** - the latitude where the meter was disengaged\n",
    " - **store_and_fwd_flag** - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server (Y=store and forward; N=not a store and forward trip)\n",
    " - **trip_duration** - duration of the trip in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
    "train['pickup_date'] = train['pickup_datetime'].dt.date\n",
    "train['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n",
    "train['store_and_fwd_flag'] = train.store_and_fwd_flag == 'Y'\n",
    "\n",
    "test['store_and_fwd_flag'] = test.store_and_fwd_flag == 'Y'\n",
    "test['pickup_date'] = test['pickup_datetime'].dt.date\n",
    "test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train['trip_duration'].values, bins=100)\n",
    "#plt.hist(train['trip_duration'].loc[train['trip_duration']<9000].values, bins=100)\n",
    "plt.xlabel('log(trip_duration)')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если решаете соревнование, может быть ползеным проверить, насколко трэйн похож на тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train.groupby('pickup_date').count()[['id']], 'o-', label='train')\n",
    "plt.plot(test.groupby('pickup_date').count()[['id']], 'o-', label='test')\n",
    "plt.title('Число поездок по дням')\n",
    "plt.legend()\n",
    "plt.ylabel('Число поездок')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30**4\n",
    "city_long_border = (-74.03, -73.75)\n",
    "city_lat_border = (40.63, 40.85)\n",
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(10,16))\n",
    "ax[0].scatter(train['pickup_longitude'].values[:N], train['pickup_latitude'].values[:N],\n",
    "              color='blue', s=1, label='train', alpha=0.1)\n",
    "ax[1].scatter(test['pickup_longitude'].values[:N], test['pickup_latitude'].values[:N],\n",
    "              color='green', s=1, label='test', alpha=0.1)\n",
    "fig.suptitle('Распределение поездок по городу.')\n",
    "ax[0].legend(loc=0)\n",
    "ax[0].set_ylabel('latitude')\n",
    "ax[0].set_xlabel('longitude')\n",
    "ax[1].set_xlabel('longitude')\n",
    "ax[1].legend(loc=0)\n",
    "plt.ylim(city_lat_border)\n",
    "plt.xlim(city_long_border)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    r = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * r * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "\n",
    "train.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n",
    "train.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n",
    "\n",
    "test.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n",
    "test.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n",
    "\n",
    "\n",
    "train.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\n",
    "train.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\n",
    "train.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\n",
    "train.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n",
    "train.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n",
    "\n",
    "test.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\n",
    "test.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\n",
    "test.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\n",
    "test.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n",
    "test.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['trip_duration']\n",
    "train.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['trip_duration']\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, sharey=True, figsize=(10,8))\n",
    "ax[0].plot(train.groupby('pickup_hour').mean()['avg_speed_h'], 'bo-', lw=2, alpha=0.7)\n",
    "ax[1].plot(train.groupby('pickup_weekday').mean()['avg_speed_h'], 'go-', lw=2, alpha=0.7)\n",
    "ax[2].plot(train.groupby('pickup_week_hour').mean()['avg_speed_h'], 'ro-', lw=2, alpha=0.7)\n",
    "ax[0].set_xlabel('hour')\n",
    "ax[1].set_xlabel('weekday')\n",
    "ax[2].set_xlabel('weekhour')\n",
    "ax[0].set_ylabel('average speed')\n",
    "fig.suptitle('Rush hour average traffic speed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_f = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime',\n",
    "                           'trip_duration', 'pickup_date', 'avg_speed_h', 'avg_speed_m']\n",
    "feature_names = [f for f in train.columns if f not in drop_f]\n",
    "\n",
    "old_feat = [f for f in old_features if f not in drop_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log = np.log(y + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xv, ytr, yv = train_test_split(train[old_feat].values, y_log, test_size=0.2, random_state=1987)\n",
    "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "dvalid = xgb.DMatrix(Xv, label=yv)\n",
    "dtest = xgb.DMatrix(test[old_feat].values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n",
    "            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(xgb_pars, dtrain, 60, watchlist, early_stopping_rounds=50,\n",
    "                  maximize=False, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modeling RMSE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y_log, test_size=0.2, random_state=1987)\n",
    "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "dvalid = xgb.DMatrix(Xv, label=yv)\n",
    "dtest = xgb.DMatrix(test[feature_names].values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgb.train(xgb_pars, dtrain, 60, watchlist, early_stopping_rounds=50,\n",
    "                  maximize=False, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modeling RMSE %.5f' % model2.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "792px",
    "right": "20px",
    "top": "166px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
